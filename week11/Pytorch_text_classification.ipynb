{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Text classification with the torchtext library\n",
    "==================================\n",
    "\n",
    "In this tutorial, we will show how to use the torchtext library to build the dataset for the text classification analysis. Users will have the flexibility to\n",
    "\n",
    "   - Access to the raw data as an iterator\n",
    "   - Build data processing pipeline to convert the raw text strings into ``torch.Tensor`` that can be used to train the model\n",
    "   - Shuffle and iterate the data with `torch.utils.data.DataLoader <https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader>`__\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Access to the raw dataset iterators\n",
    "-----------------------------------\n",
    "\n",
    "The torchtext library provides a few raw dataset iterators, which yield the raw text strings. For example, the ``AG_NEWS`` dataset iterators yield the raw data as a tuple of label and text. \n",
    "\n",
    "AG is a collection of more than 1 million news articles. News articles have been gathered from more than 2000  news sources by ComeToMyHead in more than 1 year of activity. ComeToMyHead is an academic news search engine which has been running since July, 2004. The dataset is provided by the academic comunity for research purposes in data mining (clustering, classification, etc), information retrieval (ranking, search, etc), xml, data compression, data streaming, and any other non-commercial activity. For more information, please refer to the link http://www.di.unipi.it/~gulli/AG_corpus_of_news_articles.html .\n",
    "\n",
    "The AG's news topic classification dataset used in this tutorial is constructed by Xiang Zhang (xiang.zhang@nyu.edu) from the dataset above. It is used as a text classification benchmark in the following paper: Xiang Zhang, Junbo Zhao, Yann LeCun. Character-level Convolutional Networks for Text Classification. Advances in Neural Information Processing Systems 28 (NIPS 2015).\n",
    "\n",
    "\n",
    "DESCRIPTION\n",
    "\n",
    "The AG's news topic classification dataset is constructed by choosing 4 largest classes from the original corpus. Each class contains 30,000 training samples and 1,900 testing samples. The total number of training samples is 120,000 and testing 7,600.\n",
    "\n",
    "The class labels are:\n",
    "- World\n",
    "- Sports\n",
    "- Business\n",
    "- Sci/Tech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "train.csv: 29.5MB [00:00, 67.0MB/s]                            \n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchtext.datasets import AG_NEWS\n",
    "train_iter = AG_NEWS(split='train')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset is loaded as an \"iterator\" type, meaning that you can use it directly in a for loop. To access a single item of an iterator we can use the `next` syntax in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " \"Wall St. Bears Claw Back Into the Black (Reuters) Reuters - Short-sellers, Wall Street's dwindling\\\\band of ultra-cynics, are seeing green again.\")"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3,\n",
       " 'Carlyle Looks Toward Commercial Aerospace (Reuters) Reuters - Private investment firm Carlyle Group,\\\\which has a reputation for making well-timed and occasionally\\\\controversial plays in the defense industry, has quietly placed\\\\its bets on another part of the market.')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(train_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "120000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# iterators accept the `len` method as well and return the number of items\n",
    "len(train_iter)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare data processing pipelines\n",
    "---------------------------------\n",
    "\n",
    "\n",
    "Here is an example for typical NLP data processing with tokenizer and vocabulary. The first step is to build a vocabulary with the raw training dataset. This can be done in 3 steps:\n",
    "- Tokenizing the text. You can either use the \"basic_english\" tokenizer which normalizes the text and splits it by spaces. Alternatively, you can write a custome tokenizer function and/or use spacy tokenizer and pass it to the `get_tokenizer` function. \n",
    "- Use the `Counter` method from `collections` to keep track of the count of each word in the vocabulary.\n",
    "- Create the vocabulary given the dictionary of the tokens with their counts. You can also customize the vocabulary by setting up arguments in the constructor of the Vocab class. For example, the minimum frequency ``min_freq`` for the tokens to be included.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchtext.data.utils import get_tokenizer\n",
    "from collections import Counter\n",
    "from torchtext.vocab import Vocab\n",
    "\n",
    "tokenizer = get_tokenizer('basic_english')\n",
    "train_iter = AG_NEWS(split='train')\n",
    "counter = Counter()\n",
    "for (label, line) in train_iter:\n",
    "    counter.update(tokenizer(line))\n",
    "vocab = Vocab(counter, min_freq=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['here', 'is', 'an', 'example', '!']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# the tokenizer normalize and splits the sentence into tokens\n",
    "tokenizer(\"Here is an EXAMPLE!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 22, 31, 5298, 765]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# The vocabulary block converts a list of tokens into integers.\n",
    "[vocab[token] for token in ['here', 'is', 'an', 'example', '!']]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Prepare the text processing pipeline with the tokenizer and vocabulary. The text and label pipelines will be used to process the raw data strings from the dataset iterators.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_pipeline = lambda x: [vocab[token] for token in tokenizer(x)]\n",
    "label_pipeline = lambda x: int(x) - 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text pipeline converts a text string into a list of integers based on the lookup table defined in the vocabulary. The label pipeline converts the label into integers. For example,\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[476, 22, 31, 5298]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_pipeline('here is an example')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_pipeline('4')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate data batch and iterator \n",
    "--------------------------------\n",
    "\n",
    "[`torch.utils.data.DataLoader`](https://pytorch.org/docs/stable/data.html?highlight=dataloader#torch.utils.data.DataLoader)\n",
    "is recommended for PyTorch users (a tutorial is [here]( https://pytorch.org/tutorials/beginner/data_loading_tutorial.html)).\n",
    "It works with a map-style dataset that implements the ``getitem()`` and ``len()`` protocols, and represents a map from indices/keys to data samples. It also works with an iterable dataset with the shuffle argument of ``False``.\n",
    "\n",
    "Before sending to the model, ``collate_fn`` function works on a batch of samples generated from ``DataLoader``. The input to ``collate_fn`` is a batch of data with the batch size in ``DataLoader``, and ``collate_fn`` processes them according to the data processing pipelines declared previously. Pay attention here and make sure that ``collate_fn`` is declared as a top level def. This ensures that the function is available in each worker.\n",
    "\n",
    "In this example, the text entries in the original data batch input are packed into a list and concatenated as a single tensor for the input of ``nn.EmbeddingBag``. The offset is a tensor of delimiters to represent the beginning index of the individual sequence in the text tensor. Label is a tensor saving the labels of indidividual text entries.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def collate_batch(batch):\n",
    "    label_list, text_list, offsets = [], [], [0]\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(label_pipeline(_label))\n",
    "        processed_text = torch.tensor(text_pipeline(_text), dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        offsets.append(processed_text.size(0))\n",
    "    label_list = torch.tensor(label_list, dtype=torch.int64)\n",
    "    offsets = torch.tensor(offsets[:-1]).cumsum(dim=0)\n",
    "    text_list = torch.cat(text_list)\n",
    "    return label_list.to(device), text_list.to(device), offsets.to(device)    \n",
    "\n",
    "train_iter = AG_NEWS(split='train')\n",
    "dataloader = DataLoader(train_iter, batch_size=8, shuffle=False, collate_fn=collate_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " tensor([  432,   426,     2,  1606, 14839,   114,    67,     3,   849,    14,\n",
       "            28,    15,    28,    16, 50726,     4,   432,   375,    17,    10,\n",
       "         67508,     7, 52259,     4,    43,  4010,   784,   326,     2, 15875,\n",
       "          1073,   855,  1311,  4251,    14,    28,    15,    28,    16,   930,\n",
       "           798,   321, 15875,    99,     4, 27658,    29,     6,  4460,    12,\n",
       "           565, 52791,     9, 80618,  2126,     8,     3,   526,   242,     4,\n",
       "            29,  3891, 82815,  6575,    11,   207,   360,     7,     3,   127,\n",
       "             2,    59,     9,   348,  4583,   152,    17,   739,    14,    28,\n",
       "            15,    28,    16,  2385,   453,    93,  2060, 27361,     3,   348,\n",
       "             9,     3,   739,    12,   272,    43,   241, 51954,    39,     3,\n",
       "           295,   127,   113,    86,   221,     3,  7857,     7, 40067, 15381,\n",
       "             2,    71,  7377,    59,  1811,    30,   906,   538,  2847,    14,\n",
       "            28,    15,    28,    16,   839,    40,  4979,    59, 68872,    30,\n",
       "             3,   906,  2847,     8,   538,    71, 58875,   704,     6,   913,\n",
       "          2521,    94, 89172,     4,    31,    59,   294,    27,    11,   115,\n",
       "             2,    59,    93,  4380,     5,  3582,   146,     4,  7578,    24,\n",
       "         12283,     5,    37,   348,    14,   106,    15,   106,    16, 90057,\n",
       "            51,    59,    93,     4, 11313,  1733,     9, 13751,  9736,     4,\n",
       "          3594,     6,    24,   366, 12283,  3471,    95,   300,   168,     3,\n",
       "            37,   400,   546,     2,   152,   153,    44,     4,    46,   356,\n",
       "            72,  2281,    14,    28,    15,    28,    16,   152,   790,  1358,\n",
       "           281,    11, 70412,  4434,   356,  2281,    12,     3,    72,    20,\n",
       "            59,    93,  2302,   354,   469, 55935,   716,     4, 12934,     6,\n",
       "          1618,   739,    30,   180, 77321,    65,     2,    14,   847,     2,\n",
       "           372,    15,   757,  1208,   440,     8,   308,    86,    14,    32,\n",
       "            15,    32,    16,  1767,     7,     3,   408,    17,    10,   833,\n",
       "           757,   127,  2146,  1208,   440,    25,   469,   109,     2,   783,\n",
       "           140,     8,     3,   308,    86,     5,   469, 56749,     2,  8875,\n",
       "          6853,     4,     3,   798,    55,  3008,    27,    61,     2,  1356,\n",
       "          1237,   518, 13946,    39,  1417,    14,  2200,     2,   173,    15,\n",
       "          2200,     2,   173,    16,   833,   125,  5952,   114,     6,  2540,\n",
       "             8,  1233,     4,     9,    24,   572,    12,  2445,  1688,   440,\n",
       "            70,    86,     4,     3,   101,    27,    61,     4,  7127,     3,\n",
       "           348,    22,  2567,    30,     6, 26471,  3677,     2]),\n",
       " tensor([  0,  29,  71, 111, 151, 194, 242, 289]))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(dataloader))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define the model\n",
    "----------------\n",
    "\n",
    "The model is composed of the [`nn.EmbeddingBag`](https://pytorch.org/docs/stable/nn.html?highlight=embeddingbag#torch.nn.EmbeddingBag) layer plus a linear layer for the classification purpose. ``nn.EmbeddingBag`` with the default mode of \"mean\" computes the mean value of a “bag” of embeddings. Although the text entries here have different lengths, nn.EmbeddingBag module requires no padding here since the text lengths are saved in offsets.\n",
    "\n",
    "Additionally, since ``nn.EmbeddingBag`` accumulates the average across\n",
    "the embeddings on the fly, ``nn.EmbeddingBag`` can enhance the\n",
    "performance and memory efficiency to process a sequence of tensors.\n",
    "\n",
    "![](img/model.png)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class TextClassificationModel(nn.Module):\n",
    "\n",
    "    def __init__(self, vocab_size, embed_dim, num_class):\n",
    "        super(TextClassificationModel, self).__init__()\n",
    "        self.embedding = nn.EmbeddingBag(vocab_size, embed_dim, sparse=True)\n",
    "        self.fc = nn.Linear(embed_dim, num_class)\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        initrange = 0.5\n",
    "        self.embedding.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.weight.data.uniform_(-initrange, initrange)\n",
    "        self.fc.bias.data.zero_()\n",
    "\n",
    "    def forward(self, text, offsets):\n",
    "        embedded = self.embedding(text, offsets)\n",
    "        return self.fc(embedded)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate an instance\n",
    "--------------------\n",
    "\n",
    "The ``AG_NEWS`` dataset has four labels and therefore the number of classes is four.\n",
    "\n",
    "::\n",
    "\n",
    "   1 : World\n",
    "   2 : Sports\n",
    "   3 : Business\n",
    "   4 : Sci/Tec\n",
    "\n",
    "We build a model with the embedding dimension of 64. The vocab size is equal to the length of the vocabulary instance. The number of classes is equal to the number of labels,\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iter = AG_NEWS(split='train')\n",
    "num_class = len(set([label for (label, text) in train_iter]))\n",
    "vocab_size = len(vocab)\n",
    "emsize = 64\n",
    "model = TextClassificationModel(vocab_size, emsize, num_class).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define functions to train the model and evaluate results.\n",
    "---------------------------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def train(dataloader):\n",
    "    model.train()\n",
    "    total_acc, total_count = 0, 0\n",
    "    log_interval = 500\n",
    "    start_time = time.time()\n",
    "\n",
    "    for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "        optimizer.zero_grad()\n",
    "        predited_label = model(text, offsets)\n",
    "        loss = criterion(predited_label, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.1)\n",
    "        optimizer.step()\n",
    "        total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "        total_count += label.size(0)\n",
    "        if idx % log_interval == 0 and idx > 0:\n",
    "            elapsed = time.time() - start_time\n",
    "            print('| epoch {:3d} | {:5d}/{:5d} batches '\n",
    "                  '| accuracy {:8.3f}'.format(epoch, idx, len(dataloader),\n",
    "                                              total_acc/total_count))\n",
    "            total_acc, total_count = 0, 0\n",
    "            start_time = time.time()\n",
    "\n",
    "def evaluate(dataloader):\n",
    "    model.eval()\n",
    "    total_acc, total_count = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text, offsets) in enumerate(dataloader):\n",
    "            predited_label = model(text, offsets)\n",
    "            loss = criterion(predited_label, label)\n",
    "            total_acc += (predited_label.argmax(1) == label).sum().item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc/total_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset and run the model\n",
    "-----------------------------------\n",
    "\n",
    "Since the original ``AG_NEWS`` has no validation dataset, we split the training\n",
    "dataset into train/validation sets with a split ratio of 0.95 (train) and\n",
    "0.05 (validation). Here we use\n",
    "[`torch.utils.data.dataset.random_split`](https://pytorch.org/docs/stable/data.html?highlight=random_split#torch.utils.data.random_split)\n",
    "function in PyTorch core library.\n",
    "\n",
    "[`CrossEntropyLoss`](https://pytorch.org/docs/stable/nn.html?highlight=crossentropyloss#torch.nn.CrossEntropyLoss)\n",
    "criterion combines ``nn.LogSoftmax()`` and ``nn.NLLLoss()`` in a single class.\n",
    "It is useful when training a classification problem with C classes.\n",
    "[`SGD`](https://pytorch.org/docs/stable/_modules/torch/optim/sgd.html)\n",
    "implements stochastic gradient descent method as the optimizer.\n",
    "[`StepLR`](https://pytorch.org/docs/master/_modules/torch/optim/lr_scheduler.html#StepLR)\n",
    "is used here to adjust the learning rate through epochs.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "test.csv: 1.86MB [00:00, 19.5MB/s]                  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| epoch   1 |   500/ 1782 batches | accuracy    0.686\n",
      "| epoch   1 |  1000/ 1782 batches | accuracy    0.858\n",
      "| epoch   1 |  1500/ 1782 batches | accuracy    0.874\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   1 | time: 20.87s | valid accuracy    0.885 \n",
      "-----------------------------------------------------------\n",
      "| epoch   2 |   500/ 1782 batches | accuracy    0.901\n",
      "| epoch   2 |  1000/ 1782 batches | accuracy    0.896\n",
      "| epoch   2 |  1500/ 1782 batches | accuracy    0.902\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   2 | time: 20.54s | valid accuracy    0.896 \n",
      "-----------------------------------------------------------\n",
      "| epoch   3 |   500/ 1782 batches | accuracy    0.914\n",
      "| epoch   3 |  1000/ 1782 batches | accuracy    0.914\n",
      "| epoch   3 |  1500/ 1782 batches | accuracy    0.913\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   3 | time: 21.63s | valid accuracy    0.902 \n",
      "-----------------------------------------------------------\n",
      "| epoch   4 |   500/ 1782 batches | accuracy    0.926\n",
      "| epoch   4 |  1000/ 1782 batches | accuracy    0.922\n",
      "| epoch   4 |  1500/ 1782 batches | accuracy    0.921\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   4 | time: 23.40s | valid accuracy    0.904 \n",
      "-----------------------------------------------------------\n",
      "| epoch   5 |   500/ 1782 batches | accuracy    0.929\n",
      "| epoch   5 |  1000/ 1782 batches | accuracy    0.931\n",
      "| epoch   5 |  1500/ 1782 batches | accuracy    0.929\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   5 | time: 21.98s | valid accuracy    0.907 \n",
      "-----------------------------------------------------------\n",
      "| epoch   6 |   500/ 1782 batches | accuracy    0.937\n",
      "| epoch   6 |  1000/ 1782 batches | accuracy    0.934\n",
      "| epoch   6 |  1500/ 1782 batches | accuracy    0.934\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   6 | time: 20.83s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n",
      "| epoch   7 |   500/ 1782 batches | accuracy    0.944\n",
      "| epoch   7 |  1000/ 1782 batches | accuracy    0.938\n",
      "| epoch   7 |  1500/ 1782 batches | accuracy    0.936\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   7 | time: 20.99s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n",
      "| epoch   8 |   500/ 1782 batches | accuracy    0.953\n",
      "| epoch   8 |  1000/ 1782 batches | accuracy    0.950\n",
      "| epoch   8 |  1500/ 1782 batches | accuracy    0.951\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   8 | time: 28.13s | valid accuracy    0.909 \n",
      "-----------------------------------------------------------\n",
      "| epoch   9 |   500/ 1782 batches | accuracy    0.953\n",
      "| epoch   9 |  1000/ 1782 batches | accuracy    0.953\n",
      "| epoch   9 |  1500/ 1782 batches | accuracy    0.954\n",
      "-----------------------------------------------------------\n",
      "| end of epoch   9 | time: 46.24s | valid accuracy    0.911 \n",
      "-----------------------------------------------------------\n",
      "| epoch  10 |   500/ 1782 batches | accuracy    0.952\n",
      "| epoch  10 |  1000/ 1782 batches | accuracy    0.956\n",
      "| epoch  10 |  1500/ 1782 batches | accuracy    0.954\n",
      "-----------------------------------------------------------\n",
      "| end of epoch  10 | time: 45.17s | valid accuracy    0.910 \n",
      "-----------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data.dataset import random_split\n",
    "# Hyperparameters\n",
    "EPOCHS = 10 # epoch\n",
    "LR = 5  # learning rate\n",
    "BATCH_SIZE = 64 # batch size for training\n",
    "  \n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=LR)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, 1.0, gamma=0.1)\n",
    "total_accu = None\n",
    "train_iter, test_iter = AG_NEWS()\n",
    "train_dataset = list(train_iter)\n",
    "test_dataset = list(test_iter)\n",
    "num_train = int(len(train_dataset) * 0.95)\n",
    "split_train_, split_valid_ = \\\n",
    "    random_split(train_dataset, [num_train, len(train_dataset) - num_train])\n",
    "\n",
    "train_dataloader = DataLoader(split_train_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "valid_dataloader = DataLoader(split_valid_, batch_size=BATCH_SIZE,\n",
    "                              shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE,\n",
    "                             shuffle=True, collate_fn=collate_batch)\n",
    "\n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "    epoch_start_time = time.time()\n",
    "    train(train_dataloader)\n",
    "    accu_val = evaluate(valid_dataloader)\n",
    "    if total_accu is not None and total_accu > accu_val:\n",
    "        scheduler.step()\n",
    "    else:\n",
    "        total_accu = accu_val\n",
    "    print('-' * 59)\n",
    "    print('| end of epoch {:3d} | time: {:5.2f}s | '\n",
    "          'valid accuracy {:8.3f} '.format(epoch,\n",
    "                                           time.time() - epoch_start_time,\n",
    "                                           accu_val))\n",
    "    print('-' * 59)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate the model with test dataset\n",
    "------------------------------------\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Checking the results of the test dataset…\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the results of test dataset.\n",
      "test accuracy    0.908\n"
     ]
    }
   ],
   "source": [
    "print('Checking the results of test dataset.')\n",
    "accu_test = evaluate(test_dataloader)\n",
    "print('test accuracy {:8.3f}'.format(accu_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test on a random news\n",
    "---------------------\n",
    "\n",
    "Use the best model so far and test a recent article from \"The economist\".\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is a Business news\n"
     ]
    }
   ],
   "source": [
    "ag_news_label = {1: \"World\",\n",
    "                 2: \"Sports\",\n",
    "                 3: \"Business\",\n",
    "                 4: \"Sci/Tec\"}\n",
    "\n",
    "def predict(text, text_pipeline):\n",
    "    with torch.no_grad():\n",
    "        text = torch.tensor(text_pipeline(text))\n",
    "        output = model(text, torch.tensor([0]))\n",
    "        return output.argmax(1).item() + 1\n",
    "\n",
    "ex_text_str = \"The implosion of Archegos Capital, a New York-based investment firm, \\\n",
    "            in April splashed egg on many faces. Banks that had lent it vast sums to \\\n",
    "            bet on volatile stocks have revealed over $10bn in related losses in recent weeks. \\\n",
    "            America’s leading investment banks, barring Morgan Stanley, were largely absent \\\n",
    "            from the big casualties, though. Instead the grim league table featured foreign champions. \\\n",
    "            Most notable, because of its huge loss of $5.4bn, was Credit Suisse, a Swiss bank; \\\n",
    "            also among them were ubs, its compatriot, and Nomura and Mitsubishi ufj Financial Group, \\\n",
    "            two Japanese banks.\"\n",
    "\n",
    "model = model.to(\"cpu\")\n",
    "\n",
    "print(\"This is a %s news\" %ag_news_label[predict(ex_text_str, text_pipeline)])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

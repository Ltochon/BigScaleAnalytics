{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fleet-potter",
   "metadata": {},
   "source": [
    "# Predicting house prices with neural networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sunset-blast",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "likely-interaction",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ruled-spider",
   "metadata": {},
   "source": [
    "### loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "grave-snake",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv(\"data/train.csv\")\n",
    "raw_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adjacent-substance",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "annual-officer",
   "metadata": {},
   "source": [
    "### Extracting the numeric columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "identified-wildlife",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rubber-corrections",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns = list(raw_data.columns[(raw_data.dtypes==np.int64) |\n",
    "                 (raw_data.dtypes==np.float64)])\n",
    "print(numeric_columns, \"\\n\", len(numeric_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "announced-incidence",
   "metadata": {},
   "source": [
    "Set `SalesPrice` as the last index, since it is the value we want to predict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intermediate-cable",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns.remove('SalePrice')\n",
    "numeric_columns.append('SalePrice')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "extended-attribute",
   "metadata": {},
   "source": [
    "We do not need the `Id` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "marked-cancellation",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_columns.remove('Id')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "returning-vertex",
   "metadata": {},
   "source": [
    "Now we extract the numeric data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "irish-probe",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = raw_data[numeric_columns]\n",
    "numeric_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "warming-cartoon",
   "metadata": {},
   "source": [
    "Now let's deal with the missing values in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "obvious-rider",
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_columns = np.any(pd.isna(numeric_data), axis = 0)\n",
    "nan_columns = list(nan_columns[nan_columns == True].index)\n",
    "nan_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "activated-center",
   "metadata": {},
   "source": [
    "We simply replace them with zero."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-murray",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data['LotFrontage'] = numeric_data['LotFrontage'].fillna(0)\n",
    "numeric_data['MasVnrArea'] = numeric_data['MasVnrArea'].fillna(0)\n",
    "numeric_data['GarageYrBlt'] = numeric_data['GarageYrBlt'].fillna(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "joined-grade",
   "metadata": {},
   "source": [
    "let's split the data for training and test!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-welsh",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "numeric_data_train, numeric_data_test = train_test_split(numeric_data, test_size=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-candy",
   "metadata": {},
   "source": [
    "### Normalizing the data\n",
    "Before training our linear regression model, we have to normalize the data. We do this by subtracting each column from its minimum value and then dividing it by the difference between maximum and minimum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dominant-thanksgiving",
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving max, min for each column\n",
    "maxs, mins = dict(), dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pursuant-class",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in numeric_data:\n",
    "    maxs[col] = numeric_data_train[col].max()\n",
    "    mins[col] = numeric_data_train[col].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "resistant-bathroom",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data_train = (numeric_data_train - numeric_data_train.min()) / (numeric_data_train.max() - numeric_data_train.min())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sensitive-notion",
   "metadata": {},
   "source": [
    "## Building a Linear Regression model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "insured-reviewer",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "perfect-promise",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_x_columns = list(numeric_data_train.columns)\n",
    "numeric_x_columns.remove(\"SalePrice\")\n",
    "X_train_df = numeric_data_train[numeric_x_columns]\n",
    "y_train_df = pd.DataFrame(numeric_data_train[\"SalePrice\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wanted-surge",
   "metadata": {},
   "source": [
    "Now we have to convert the data into torch tensors. A `torch.Tensor` is a multi-dimensional matrix containing elements of a single data type. It's very similar to arrays in `NumPy`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ranging-hudson",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = torch.tensor(X_train_df.values, dtype=torch.float)\n",
    "y_train = torch.tensor(y_train_df.values, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prescribed-reason",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.size(), y_train.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baking-jamaica",
   "metadata": {},
   "source": [
    "### Defining a model with pytorch\n",
    "A model is always defined as a class in pytorch. It should have a `__init__` function in which you define the layers of your network. It also should have a `forward` function (method) that basically defines the forward pass on the network.\n",
    "\n",
    "For the beggining, let's start with a single layer network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "written-motel",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        \n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, D_out)\n",
    "        self.activation = nn.ReLU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "        y_pred = self.activation(self.linear1(x))\n",
    "        y_pred = self.linear2(y_pred)\n",
    "        return y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sustainable-blade",
   "metadata": {},
   "outputs": [],
   "source": [
    "D_in, D_out = X_train.shape[1], y_train.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "subjective-camcorder",
   "metadata": {},
   "outputs": [],
   "source": [
    "# defining the first model: an instance of the class \"Net\"\n",
    "model1 = Net(D_in, 500, D_out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sticky-flash",
   "metadata": {},
   "source": [
    "The next steps is to define the __loss criterion__ and the __optimizer__ for the network. That is, we have to define the loss function we want to optimize during training and also the optimization method we are going to use, e.g, SGD, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "permanent-fancy",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "# SGD optimizer for finding the weights of the network\n",
    "optimizer = torch.optim.SGD(model1.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "individual-gentleman",
   "metadata": {},
   "source": [
    "Now, we are ready to do the training. We can simply do this by a for loop over the number of iterations. The training has 3 main steps:\n",
    "- A forward pass to compute the prediction for the current data point (batch).\n",
    "- computing the loss for the current prediction.\n",
    "- A backward pass to compute the gradient of the loss with respect to the weight of the network.\n",
    "- Finaly, updating the weights of the network (`optimizer.step()`).\n",
    "\n",
    "Note that in each backward pass pytorch saves the gradient for all of the parameters. Therefore it is important to replace the old gradient values with zero in the beggining of each iteration, otherwise the gradients will be accumulated during the iterations!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "earlier-maximum",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses1 = []\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model1(X_train)\n",
    "    \n",
    "    loss = criterion(y_pred, y_train)\n",
    "    print(t, loss.item())\n",
    "    losses1.append(loss.item())\n",
    "    \n",
    "    if torch.isnan(loss):\n",
    "        break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "coordinated-mexican",
   "metadata": {},
   "source": [
    "Now let's try a new model with more neurons in the hidden layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cathedral-exercise",
   "metadata": {},
   "outputs": [],
   "source": [
    "model2 = Net(D_in, 1000, D_out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exposed-league",
   "metadata": {},
   "outputs": [],
   "source": [
    "# MSE loss\n",
    "criterion = nn.MSELoss(reduction='sum')\n",
    "# SGD optimizer for finding the weights of the network\n",
    "optimizer = torch.optim.SGD(model2.parameters(), lr=1e-4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "operational-paper",
   "metadata": {},
   "outputs": [],
   "source": [
    "losses2 = []\n",
    "\n",
    "for t in range(500):\n",
    "    y_pred = model2(X_train)\n",
    "    \n",
    "    loss = criterion(y_pred, y_train)\n",
    "    print(t, loss.item())\n",
    "    losses2.append(loss.item())\n",
    "    \n",
    "    if torch.isnan(loss):\n",
    "        break\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extra-consortium",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(losses1, label=\"model1\")\n",
    "plt.plot(losses2, label=\"model2\")\n",
    "plt.ylim([0, 70])\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "banned-honolulu",
   "metadata": {},
   "source": [
    "Let's compare the MSE loss on the test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "norwegian-mayor",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to normalize the test data with the min and max value\n",
    "# from the training data\n",
    "for col in numeric_data_test.columns:\n",
    "    numeric_data_test[col] = (numeric_data_test[col] - mins[col]) / (maxs[col] - mins[col])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "parallel-following",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_df = pd.DataFrame(numeric_data_test[\"SalePrice\"])\n",
    "y_test = torch.tensor(y_test_df.values, dtype=torch.float)\n",
    "x_test_df = numeric_data_test[numeric_x_columns]\n",
    "x_test = torch.tensor(x_test_df.values, dtype=torch.float)\n",
    "# prediction for model 1\n",
    "model1_pred = model1(x_test)\n",
    "print(\"MSE loss for model1: \", criterion(model1_pred, y_test))\n",
    "# prediction for model 2\n",
    "model2_pred = model2(x_test)\n",
    "print(\"MSE loss for model2: \", criterion(model2_pred, y_test))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wrong-corporation",
   "metadata": {},
   "source": [
    "## Now it is your turn!\n",
    "### Exercises\n",
    "\n",
    "1- Let's get back to model1. This time try to train it with a new optimizer. Try the Adam optimizer (which has shown to be faster than SGD for non-convex functions) and compare the trainig loss curve with SGD. Plot the training loss for the model trained with SGD and Adam optimizer.\n",
    "\n",
    "Note1: Use `torch.optim.Adam(model1.parameters(), lr=...)`\n",
    "\n",
    "Note2: If you are interested, check [this nice post](https://ruder.io/optimizing-gradient-descent/index.html) on differen gradient descent optimization algorithms.\n",
    "\n",
    "2- This time we want to build a new model with a new architecture. Specifically, we want to train a network with 3 hidden layers on the data. You can use the following code to build the architecture. Use the values 500, 1000, 200 for H1, H2, and H3 respectively. Train this new network on the same training data and compare it with the model1 we built above.\n",
    "\n",
    "```\n",
    "class Net_new(nn.Module):\n",
    "    def __init__(self, D_in, H1, H2, H3, D_out):\n",
    "        super(Net_new, self).__init__()\n",
    "\n",
    "        self.linear1 = nn.Linear(D_in, H1)\n",
    "        self.linear2 = nn.Linear(H1, H2)\n",
    "        self.linear3 = nn.Linear(H2, H3)\n",
    "        self.linear4 = nn.Linear(H3, D_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear1(x).clamp(min=0)\n",
    "        y_pred = self.linear2(y_pred).clamp(min=0)\n",
    "        y_pred = self.linear3(y_pred).clamp(min=0)\n",
    "        y_pred = self.linear4(y_pred)\n",
    "        return y_pred\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "respiratory-drink",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

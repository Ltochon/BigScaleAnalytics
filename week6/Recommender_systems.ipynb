{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Recommender_systems.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/michalis0/BigScaleAnalytics/blob/master/week6/Recommender_systems.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "12cW8RItmOpj"
      },
      "source": [
        "# Data Mining and Machine Learning - Week 6\n",
        "# Recommender Systems\n",
        "\n",
        "A [recommender system](https://en.wikipedia.org/wiki/Recommender_system) is a subclass of information filtering system that seeks to predict the \"rating\" or \"preference\" a user would give to an item. They are primarily used in commercial applications.\n",
        "\n",
        "\n",
        "### Table of Contents\n",
        "#### 1. User-User Collaborative Filtering From Scratch\n",
        "* 1.1 Download the data\n",
        "* 1.2 Import modules and load data\n",
        "* 1.3 Data visualization\n",
        "* 1.4 Truncate the data\n",
        "* 1.5 Preprocess the data\n",
        "* 1.6 Create the Ratings Matrix\n",
        "* 1.7 User Average Ratings\n",
        "* 1.8 Compute User-User Similarity \n",
        "* 1.9 User to all Users Similarities\n",
        "* 1.10 Create User Neighborhood \n",
        "* 1.11 Predict a Rating \n",
        "* 1.12 Recommendation Exercise\n",
        "\n",
        "#### 2. Using the Surprise library\n",
        "* 2.1 Some helper functions\n",
        "* 2.2 Visualizing the recommendations\n",
        "* 2.3 Cross-validation using surprise\n",
        "* 2.4 Precision - Recall @k\n",
        "* 2.5 Tuning hyper-parameters with surprise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WvwKN6fqmMZ8"
      },
      "source": [
        "## 1. User-User Collaborative Filtering From Scratch\n",
        "We implement User-User Collaborative Filtering from scratch i.e. by only using numpy and scipy libraries. We use the MovieLens 20M dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmljqGT8nAAF"
      },
      "source": [
        "### 1.1 Download the data\n",
        "We download the MovieLens 20M dataset from <http://files.grouplens.org/datasets/movielens/ml-20m.zip>, extract the contents of the zip file as a folder named `data` located in the same folder as this notebook. You should have a file `ml-20m/ratings.csv`, which is what we'll be working with."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tdBAyh9rmMZ9",
        "outputId": "1626392e-0284-4e8a-9932-329b8c4e2e85",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Download data\n",
        "!pip install wget\n",
        "!mkdir data\n",
        "import wget\n",
        "filename = wget.download(\"http://files.grouplens.org/datasets/movielens/ml-20m.zip\", out=\"data/ml-20m.zip\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting wget\n",
            "  Downloading wget-3.2.zip (10 kB)\n",
            "Building wheels for collected packages: wget\n",
            "  Building wheel for wget (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wget: filename=wget-3.2-py3-none-any.whl size=9675 sha256=c773ea0a9f03bcaabf64ff7b4cf10fece9e20dfa818ba0ef52158a1da4027739\n",
            "  Stored in directory: /root/.cache/pip/wheels/a1/b6/7c/0e63e34eb06634181c63adacca38b79ff8f35c37e3c13e3c02\n",
            "Successfully built wget\n",
            "Installing collected packages: wget\n",
            "Successfully installed wget-3.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uqK7IOPLmMZ9",
        "outputId": "dda3f457-2647-430a-ef9e-9c1e4cc0fb43",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "# Unzip the zip file to get the data-sets\n",
        "!unzip data/ml-20m.zip"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  data/ml-20m.zip\n",
            "   creating: ml-20m/\n",
            "  inflating: ml-20m/genome-scores.csv  \n",
            "  inflating: ml-20m/genome-tags.csv  \n",
            "  inflating: ml-20m/links.csv        \n",
            "  inflating: ml-20m/movies.csv       \n",
            "  inflating: ml-20m/ratings.csv      \n",
            "  inflating: ml-20m/README.txt       \n",
            "  inflating: ml-20m/tags.csv         \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzLcWslemMZ9"
      },
      "source": [
        "### 1.2 Import modules and load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ouu8oEhzmMZ9"
      },
      "source": [
        "# Import requiered packages\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from scipy import sparse as sp\n",
        "from scipy.sparse.linalg import norm\n",
        "import sklearn.preprocessing as pp\n",
        "import matplotlib.pyplot as plt\n",
        "plt.style.use('dark_background')\n",
        "\n",
        "# Set some formatting options\n",
        "np.set_printoptions(threshold=500, precision=4)\n",
        "pd.options.display.max_seq_items = 20\n",
        "pd.options.display.max_rows = 20"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1ANI5z7mMZ-"
      },
      "source": [
        "Load data from csv files."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZVjGJMVXmMZ-"
      },
      "source": [
        "ratings_raw = pd.read_csv('ml-20m/ratings.csv')\n",
        "movies = pd.read_csv('ml-20m/movies.csv')\n",
        "links = pd.read_csv('ml-20m/links.csv')"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ikvODPGbmMZ-"
      },
      "source": [
        "Let's see what the data looks like."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6mWQnnUcmMZ-"
      },
      "source": [
        "display(ratings_raw.head())\n",
        "display(movies.head())\n",
        "display(links.head())\n",
        "print(\"Distinct users:\", len(ratings_raw.userId.unique()))\n",
        "print(\"Distinct items:\", len(ratings_raw.movieId.unique()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V9zIoj9UuoW1"
      },
      "source": [
        "# Example to keep in mind for later --> id is 480\n",
        "movies[movies.title == 'Jurassic Park (1993)']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H1oYI3WxmMZ-"
      },
      "source": [
        "### 1.3 Data visualization\n",
        "Lets visualize some of the movie data. We will use [tmdbsimple](https://pypi.org/project/tmdbsimple/) which is a wrapper, written in Python, for The Movie Database (TMDb) API v3."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b3dLjYwwmMZ-"
      },
      "source": [
        "# Install package\n",
        "!pip install tmdbsimple"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "elTxW5fOn94T"
      },
      "source": [
        "We directly import a module from GitHub. It runs the `__init__.py` file, which contains a class we need called `TMDB`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T97y7f2YuQgB"
      },
      "source": [
        "# Import module from GitHub\n",
        "from httpimport import github_repo\n",
        "with github_repo(username='michalis0', repo='DataMining_and_MachineLearning', module='week6'):\n",
        "  import week6\n",
        "\n",
        "week6"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ujlt6Z-mmMZ-"
      },
      "source": [
        "from IPython.display import Image\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "# Which movies did user watch?\n",
        "def make_html(image_url):\n",
        "     return '<img src=\"{}\" style=\"display:inline;margin:1px\" width=\"100\"/>'\\\n",
        "            .format(image_url)\n",
        "        \n",
        "def show_movies_for_user(userId, verbose=False, show_all=False):\n",
        "    \"\"\"Retrieve posters of top rated movies for userId.\n",
        "    \n",
        "    Note: this accepts the original user id\n",
        "    \"\"\"\n",
        "    html = ''\n",
        "    max_movies = 10\n",
        "    i=0\n",
        "    \n",
        "    user_movies = ratings_raw[ratings_raw.userId == userId][\"movieId\"]\n",
        "    print(\"User \", userId , \" watched \", len(user_movies), ' movies') \n",
        "    \n",
        "    user_movies = ratings_raw[ratings_raw.userId == userId].sort_values(\"rating\", ascending=False) \n",
        "    for index, row in user_movies.iterrows():\n",
        "        movieId = row[\"movieId\"]\n",
        "        movie_item = links[links.movieId == movieId]\n",
        "        tmdbId = movie_item[\"tmdbId\"].item()\n",
        "        if verbose:\n",
        "            print(movieId, tmdbId)\n",
        "        if np.isnan(tmdbId):\n",
        "            url = None\n",
        "        else:\n",
        "            url = week11.TMDB().get_poster_path_by_id(int(tmdbId)) # use imported module\n",
        "        html = html + make_html(url)\n",
        "        i +=1\n",
        "        \n",
        "        if ~show_all and (i == max_movies):\n",
        "                break\n",
        "        \n",
        "    display(HTML(html))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gkYybOWomMZ_"
      },
      "source": [
        "# Let's see which movies user 6 watched\n",
        "show_movies_for_user(6, verbose=False) "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tVd_Cg_-E7oW"
      },
      "source": [
        "# Show this in ratings_raw\n",
        "ratings_raw[ratings_raw.userId == 6]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZbnQLvxDmMZ_"
      },
      "source": [
        "# Some helper functions\n",
        "def get_name_for_movie_id(movie_id):\n",
        "    \"\"\"Returns the name of a movie_id (based on ratings_raw i.e. original id)\"\"\"\n",
        "    \n",
        "    try: \n",
        "        movie_name = movies[movies.movieId == movie_id][\"title\"].item()\n",
        "    except KeyError:\n",
        "        movie_name = None\n",
        "    return movie_name\n",
        "    \n",
        "    \n",
        "def show_genres_histogram_for_user(user_id):\n",
        "    \"\"\"Create histogram of movies genres user_id has watched.\n",
        "    \n",
        "    Note: this uses the original user_id NOT the userIDX\n",
        "    \"\"\"\n",
        "    \n",
        "    user_movies = ratings_raw[ratings_raw.userId == user_id][\"movieId\"]\n",
        "    print(\"User \", user_id , \" watched \", len(user_movies), ' movies') \n",
        "    user_movies_with_genre = movies[movies.movieId.isin(user_movies)]\n",
        "    display(user_movies_with_genre)\n",
        "    \n",
        "    genres_list = []\n",
        "    for index, row in user_movies_with_genre.iterrows():\n",
        "        genres_list += row[\"genres\"].split('|')\n",
        "    \n",
        "    df_genres = pd.DataFrame(genres_list, columns=['genres'])\n",
        "    \n",
        "    df_genres.groupby('genres').size().sort_values().plot(kind='barh')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7QVi7s5mMZ_"
      },
      "source": [
        "# Now we can see what user 6 watched\n",
        "show_genres_histogram_for_user(6)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tD4Qa2yJmMZ_"
      },
      "source": [
        "### 1.4 Truncate the data\n",
        "\n",
        "To speed things up, we will work with a *truncated* version of the data, containing up to 10000 users and 1000 movies. \n",
        "\n",
        "**Important:** To see results with the full dataset, set `DEBUG = False`, and **rerun all cells starting from the top**. But be careful this may take a long time."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LF--FMQ-mMZ_"
      },
      "source": [
        "DEBUG = True\n",
        "if DEBUG: \n",
        "    ratings_raw = ratings_raw[ (ratings_raw['userId'] < 10000) & (ratings_raw['movieId'] < 1000) ]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eg6HAA8MmMZ_"
      },
      "source": [
        "### 1.5 Preprocess the data\n",
        "We make sure that movies and users have consecutive indexes starting from 0. Also drop the timestamp column.\n",
        "\n",
        "The resulting \"cleaned\" data are stored in `ratings`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OU2AlsSNmMZ_"
      },
      "source": [
        "# Unique movies and users\n",
        "movieIds = ratings_raw.movieId.unique()\n",
        "movieIds.sort()\n",
        "userIds = ratings_raw.userId.unique()\n",
        "userIds.sort()\n",
        "\n",
        "# Size\n",
        "m = userIds.size\n",
        "n = movieIds.size\n",
        "numRatings = len(ratings_raw)\n",
        "\n",
        "print (\"There are\", m, \"users,\", n, \"items and\", numRatings, \"ratings.\")\n",
        "\n",
        "## Movies and users should have consecutive indexes starting from 0\n",
        "# dictionaries to convert movie id to consecutive index and vice versa\n",
        "movieId_to_movieIDX = dict(zip(movieIds, range(0, movieIds.size)))\n",
        "movieIDX_to_movieId = dict(zip(range(0, movieIds.size), movieIds))\n",
        "\n",
        "# Dictionaries to convert user id to consecutive index and vice versa\n",
        "userId_to_userIDX = dict(zip(userIds, range(0, userIds.size )))\n",
        "userIDX_to_userId = dict(zip(range(0, userIds.size), userIds))\n",
        "\n",
        "# Drop timestamps\n",
        "ratings = pd.concat([ratings_raw['userId'].map(userId_to_userIDX),\n",
        "                     ratings_raw['movieId'].map(movieId_to_movieIDX),\n",
        "                     ratings_raw['rating']], axis=1)\n",
        "ratings.columns = ['user', 'item', 'rating']\n",
        "\n",
        "display(ratings.head(20))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S56uwMG-t0lV"
      },
      "source": [
        "We therefore have an index (IDX) and and ID for each movies (item) and each user."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jRhcUmH8o6Zq"
      },
      "source": [
        "# Example - index to id\n",
        "for idx in range(40):\n",
        "  print(idx, userIDX_to_userId[idx])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yH27TM5cpdN2"
      },
      "source": [
        "# Example - id to index\n",
        "for id in range(0, 40):\n",
        "  try:\n",
        "    print(id, userId_to_userIDX[id])\n",
        "  except:\n",
        "    print('There is nobody with id = ' + str(id))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PikTTMfE67Cp"
      },
      "source": [
        "# Describe\n",
        "ratings.groupby(by='user').describe()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q-0vV_MvmMZ_"
      },
      "source": [
        "### 1.6 Create the Ratings Matrix\n",
        "\n",
        "We convert the `ratings DataFrame` into a **Ratings Matrix**. Because it is very sparse, we use the `scipy.sparse` module to efficiently store and access it.\n",
        "\n",
        "Specifically, we create **two** versions of the same ratings matrix:\n",
        "- `R` is our basic matrix and is optimized for dot products, which will be useful when computing user-user similarities; `R` is stored in the Compressed Sparse Row format (`csr_matrix`).\n",
        "- `R_dok` is a different view of the ratings matrix, which allows to quickly test whether a user-item rating exists; `R_dok` is stored in the Dictionary Of Keys format (`dok_matrix`) so you can access the data like a dictionary (which is fast)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W0EyJSN2mMZ_"
      },
      "source": [
        "# Create matrices\n",
        "R = sp.csr_matrix((ratings.rating, (ratings.user, ratings.item))) # input is (data value, (index 0, index 1))\n",
        "R_dok = R.todok()\n",
        "\n",
        "# A simple test: user 0 item 534 should have a rating of 4\n",
        "print(\"R[0, 534] value is \", R[0, 534])\n",
        "print(\"R_dok[(0, 534)] value is \", R_dok[(0, 534)])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y0nWfDQ7mMZ_"
      },
      "source": [
        "# Check - same as before :)\n",
        "m = R.shape[0]\n",
        "n = R.shape[1]\n",
        "numRatings = R.count_nonzero()\n",
        "\n",
        "print(\"There are m =\", m, \"users, n =\", n, \"items and\", numRatings, \"ratings.\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "70Le4YNQmMZ_"
      },
      "source": [
        "The fun starts here! :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-Ov2ihAWmMZ_"
      },
      "source": [
        "### 1.7 User Average Ratings"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i2MgtshPmMZ_"
      },
      "source": [
        "The following code computes the average rating of each user. This will be useful for mean-centering, i.e., when computing similarities, as well as for making predictions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-j329hh3mMZ_"
      },
      "source": [
        "user_sums = R.sum(axis=1).A1 # matrix converted to 1-D array via .A1\n",
        "user_cnts = np.diff(R.indptr) # equivalent to, but faster than: user_cnts = (R != 0).sum(axis=1).A1\n",
        "user_avgs = user_sums / user_cnts\n",
        "print(\"user_avgs\", user_avgs)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OrcGaqpVmMZ_"
      },
      "source": [
        "[This thread](https://stackoverflow.com/questions/52299420/scipy-csr-matrix-understand-indptr) in stack overflow, explains the method `indptr` for a sparse matrix in scipy clearly. Recommended to read for those who are inerested or got confused with the above code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PcfoifKSmMZ_"
      },
      "source": [
        "### 1.8 Compute User-User Similarity \n",
        "\n",
        "The following function computes the **mean-centered cosine similarity** between two users.\n",
        "\n",
        "**IMPORTANT**: from now on, we work with `R` and `R_dok`. This means that we work with the IDX, not the ids anymore. \n",
        "\n",
        "*Tricks* that are useful:\n",
        "\n",
        "To subtract a scalar value `y` from all nonzero entries of a sparse vector `x`, do:\n",
        "```\n",
        "x.data = x.data - y\n",
        "```\n",
        "\n",
        "The dot product of a sparse vector `x` to sparse vector `y` is:\n",
        "```\n",
        "x.dot(y.T)\n",
        "```\n",
        "\n",
        "The norm of a sparse vector `x` is:\n",
        "```\n",
        "norm(x)\n",
        "```\n",
        "\n",
        "\n",
        "If a sparse vector `x` has only a single item, you can access it by:\n",
        "```\n",
        "x.A.item()\n",
        "```\n",
        "\n",
        "Note that `x.A` returns the dense representation of sparse vector `x`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z0BSmak3mMZ_"
      },
      "source": [
        "# Some examples\n",
        "u = R[1,:].copy() \n",
        "v = R[2,:].copy()\n",
        "print(type(u.data))\n",
        "print(u.dot(v.T))\n",
        "print(u.dot(v.T).A)\n",
        "print(u.dot(v.T).A.item())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-u_tGHy9mMaA"
      },
      "source": [
        "def compute_pairwise_user_similarity(u_id, v_id):\n",
        "    \"\"\"Computes the cosine similarity between two user ids.\n",
        "    dot(A,B)/norm(A)*norm(B)\n",
        "    Note: we use it with the IDX!\n",
        "    \"\"\"\n",
        "    u = R[u_id,:].copy()\n",
        "    v = R[v_id,:].copy()\n",
        "    \n",
        "    # Compute the numerator i.e. dot product of the mean centered arrays\n",
        "    u.data = u.data - user_avgs[u_id]\n",
        "    v.data = v.data - user_avgs[v_id]\n",
        "    \n",
        "    numerator =  (u.dot(v.T)).A.item()\n",
        "    \n",
        "    # Compute demoninator i.e. product of the norms\n",
        "    denominator = norm(u) * norm(v)\n",
        "    \n",
        "    # Compute similarity\n",
        "    if denominator == 0:\n",
        "        similarity = 0.;\n",
        "    else:\n",
        "        similarity = numerator/denominator\n",
        "    \n",
        "    return similarity"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bp4wOE0QmMaA"
      },
      "source": [
        "**DEBUG:** For the truncated dataset, the following should output ~ `0.03585`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZqMUMqVFmMaA"
      },
      "source": [
        "if DEBUG:\n",
        "    sim = compute_pairwise_user_similarity(2, 6)\n",
        "    print(round(sim, 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pmZt2vQEmMaA"
      },
      "source": [
        "### 1.9 User to all Users Similarities\n",
        "\n",
        "The following functions compute the mean-centered cosine similarities of a given user to all other users.\n",
        "\n",
        "We first use the `compute_pairwise_user_similarity` function defined above.\n",
        "\n",
        "Then we try to avoid the for loop and **NOT** invoke `compute_pairwise_user_similarity`. The idea is to obtain a copy, say `R_copy`, of matrix `R` that has its rows mean-centered and normalized. This way the given user can be represented by a mean-centered and normalized vector `u`. Then, to obtain the similarity of the user to all others, one needs to take the dot product `R_copy.dot(u.T)`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "myKa5Gn8mMaA"
      },
      "source": [
        "def compute_user_similarities(u_id):\n",
        "    uU = np.empty((m,)) # we have m users\n",
        "  \t\n",
        "    # With for loop\n",
        "    for v_id in range(m):\n",
        "        sim = compute_pairwise_user_similarity(u_id, v_id)\n",
        "        uU[v_id] = sim\n",
        "    \n",
        "    return uU\n",
        "\n",
        "def compute_user_similarities_fast(u_id):\n",
        "    uU = np.empty((m,))\n",
        "    \n",
        "    global user_avgs, user_cnts # we already derived these two vectors\n",
        "    \n",
        "    R_copy = R.copy() ## create a copy to work with\n",
        "    # Repeat each user_avg, user_cnt times\n",
        "    user_avgs_repeated = np.repeat(user_avgs, user_cnts)\n",
        "\n",
        "    R_copy.data -= user_avgs_repeated # R_copy is now mean centered\n",
        "    \n",
        "    # Normalize rows to unit norm\n",
        "    R_copy = pp.normalize(R_copy, axis=1) # normalize each row: elements divided by the row norm\n",
        "    u = R_copy[u_id, :]\n",
        "\n",
        "    uU = R_copy.dot(u.T).A.flatten() # dot product; convert to dense matrix, then flatten\n",
        "    return uU"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k4vu1283mMaA"
      },
      "source": [
        "**DEBUG:** For the truncated dataset, the following should again output ~ `0.03585`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EGWJcCprmMaA"
      },
      "source": [
        "import time\n",
        "if DEBUG:\n",
        "    start = time.time()\n",
        "    uU = compute_user_similarities(2)\n",
        "    print(\"Time =\", (time.time()-start))\n",
        "    \n",
        "    start = time.time()\n",
        "    uU = compute_user_similarities_fast(2)\n",
        "    print(\"Time =\", (time.time()-start))\n",
        "    \n",
        "    print(round(uU[6], 5))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0fEuvUqmMaA"
      },
      "source": [
        "print(uU.shape); print(uU) ## uU stores the similarity of that user to all the other users."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ds8gXJ-BmMaA"
      },
      "source": [
        "np.argsort(uU)[-2] # most similar user to user with user index 2. Note: we take the penultimate one since the last one is user 2 with a similarity = 1"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4peZBeTwEHuU"
      },
      "source": [
        "# Similarity between user 2 and user 1871\n",
        "uU[1871]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_wibGe5amMaA"
      },
      "source": [
        "Now let's compare the top 10 rated movies by user index 2 and user index 1871 which is the most similar user to user index 2."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "c9mYhs0ZmMaA"
      },
      "source": [
        "show_movies_for_user(userIDX_to_userId[2]) # note that we use userIDX_to_userId"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i-S5zKgFmMaA"
      },
      "source": [
        "show_movies_for_user(userIDX_to_userId[1871])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D5CkBPfGmMaA"
      },
      "source": [
        "### 1.10 Create User Neighborhood "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7kSjCeZ5mMaA"
      },
      "source": [
        "The following function creates the user neighborhood of a given user. It takes as input, the target user `u_id` and the target item `i_id`, and uses additional parameters, the size `k` of the neighborhood, and a flag `with_abs_sim`.\n",
        "\n",
        "If `with_abs_sim` is `True`, the neighborhood should contain up to `k` users with the highest absolute similarity to the target user `u_id`.\n",
        "\n",
        "If `with_abs_sim` is `False`, the neighborhood should contain up to `k` users with the highest similarity to the target user `u_id`.\n",
        "\n",
        "The output of the function is `nh`, a `Dictionary` containing key-value entries of the form `v_id : sim(u_id, v_id)`, where `v_id` is another user and `sim(u_id, v_id)` is the similarity between `u_id` and `v_id`.\n",
        "\n",
        "**Note:** The neighborhood of the target user should not contain itself, i.e., `u_id`, and only include users that have rated the target item `i_id`.\n",
        "\n",
        "\n",
        "*Tricks* that might be useful:\n",
        "\n",
        "`np.absolute(x)` returns an array containing the absolute values of each element in array `x`.\n",
        "\n",
        "`np.argsort(x)` returns an array with the indices that sort array `x` in *increasing* order.\n",
        "\n",
        "`x[::-1]` returns the reversed array of `x`. So, `np.argsort(x)[::-1]` contains the indices that sort x in *decreasing* order.\n",
        "\n",
        "To check if user `u_id` has rated item `i_id`, the `R_dok` view of the ratings matrix is helpful:\n",
        "```\n",
        "(u_id, i_id) in R_dok\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vBNqDjaDmMaA"
      },
      "source": [
        "def create_user_neighborhood(u_id, i_id, k=5, with_abs_sim=False):\n",
        "    \"\"\" Neighborhood for user \"u_id\" that have also watched item \"i_id\"\n",
        "    \"\"\"\n",
        "    nh = {} ## the neighborhood dict with (user id: similarity) entries\n",
        "    ## nh should not contain u_id and only include users that have rated i_id; there should be at most k neighbors\n",
        "    \n",
        "    uU = compute_user_similarities(u_id)\n",
        "    uU_copy = uU.copy() ## so that we can modify it, but also keep the original\n",
        "    \n",
        "    # Absolute similarity\n",
        "    if with_abs_sim:\n",
        "        uU_copy = np.absolute(uU_copy)  # we only care about the absolute value of the similarity\n",
        "    \n",
        "    # Derive the user_ids sorted by decreasing similarity (or absolute similarity) to u_id\n",
        "    user_ids =  np.argsort(uU_copy)[::-1]\n",
        "    \n",
        "    # Create neighborhood\n",
        "    count = 0\n",
        "    for v_id in user_ids:\n",
        "        if v_id == u_id: ## ignore self\n",
        "            continue # go to the next iterate of the loop \n",
        "        # ignore users that have not rated i_id\n",
        "        if (v_id, i_id) not in R_dok: \n",
        "            continue\n",
        "        nh[v_id] = uU[v_id]\n",
        "        count += 1\n",
        "        if count == k:\n",
        "            break\n",
        "    \n",
        "    return nh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "otmCwemxmMaA"
      },
      "source": [
        "# Compute neighborhood for user (idx) 2 and movie (id) 480 (Jurassic Park)\n",
        "if DEBUG:\n",
        "    print(get_name_for_movie_id(movie_id=480))\n",
        "    nh = create_user_neighborhood(u_id=2, i_id=movieId_to_movieIDX[480]) # note that we use movieId_to_movieIDX\n",
        "    print(nh)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xmbb8NsyI4lK"
      },
      "source": [
        "compute_pairwise_user_similarity(2, 1871)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K7Z6g-wRmMaA"
      },
      "source": [
        "Now we have seen what are the most similar users to user 2 that have also watched `Jurassic Park`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pPKdQSNWmMaA"
      },
      "source": [
        "# User 2\n",
        "show_movies_for_user(userIDX_to_userId[2])\n",
        "show_genres_histogram_for_user(userIDX_to_userId[2])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "xjizboG9mMaA"
      },
      "source": [
        "# Now we can see what movies the userIDX 3820 (the second most similar to user_idx=2) has watched.\n",
        "show_movies_for_user(userIDX_to_userId[3820])\n",
        "show_genres_histogram_for_user(userIDX_to_userId[3820])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SaAMZR_3mMaA"
      },
      "source": [
        "### 1.11 Predict a Rating \n",
        "\n",
        "Finally! We can now try to predict a rating of a given user who has not rated an item so far. Follow the instructions bellow to see how we can do this!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gTl-1zqlmMaA"
      },
      "source": [
        "The following function predicts the rating user `u_id` would give to item `i_id`. It uses the flag `with_deviations` to make the prediction.\n",
        "\n",
        "If `with_deviations` is `True`, the prediction is made over *rating deviations*:\n",
        "$$ s(u,i) = \\overline{r_u} + \\frac{\\sum_{v \\in N(u;i)}w_{uv} (r_{vi}-\\overline{r_v})}{\\sum_{v \\in N(u;i)} |w_{uv}|} .$$\n",
        "\n",
        "If `with_deviations` is `False`, the prediction is made directly over ratings:\n",
        "$$ s(u,i) = \\frac{\\sum_{v \\in N(u;i)}w_{uv} r_{vi}}{\\sum_{v \\in N(u;i)} |w_{uv}|} .$$\n",
        "\n",
        "The output of the function is the predicted rating `prediction`."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Sy7a_5UmMaA"
      },
      "source": [
        "def predict_rating(u_id, i_id, k, with_deviations=True, with_abs_sim=False):\n",
        "    '''\n",
        "    predict the rating of user u_id for item i_id \n",
        "    '''\n",
        "    \n",
        "    print(\"Movie name:\", get_name_for_movie_id(movie_id=movieIDX_to_movieId[i_id]))\n",
        "    \n",
        "    if (u_id, i_id) in R_dok:\n",
        "        print(\"user idx\", u_id, \"has rated item idx\", i_id, \"with\", R[u_id, i_id])\n",
        "    else:\n",
        "        print(\"user idx\", u_id, \"has not rated item idx\", i_id)\n",
        "    print(\"k:\", k, \"with_deviations:\", with_deviations, \"with_abs_sim:\", with_abs_sim)\n",
        "    \n",
        "    # Neighborhood\n",
        "    nh = create_user_neighborhood(u_id, i_id, k=k, with_abs_sim=with_abs_sim)\n",
        "    \n",
        "    \n",
        "    neighborhood_weighted_avg = 0. # neighborhood_weighted_avg is the fraction in the above formulas\n",
        "\n",
        "    # Average - same as before\n",
        "    user_sums = R.sum(axis=1).A1 ## matrix converted to 1-D array via .A1\n",
        "    user_cnts = np.diff(R.indptr) ## equivalent to, but faster than: user_cnts = (R != 0).sum(axis=1).A1\n",
        "    user_avgs = user_sums / user_cnts\n",
        "\n",
        "    # Compute fraction\n",
        "    sum_scores = 0. # numerator\n",
        "    sum_weights = 0. # denominator\n",
        "    for neighbor_id, similarity in nh.items():\n",
        "        # Find the neighbor rating from R matrix.\n",
        "        neighbor_rating = R[neighbor_id, i_id]\n",
        "        if with_deviations:\n",
        "            # In this case similarity should be multiplied by (neighbor_rating - neighbor_avg)\n",
        "            sum_scores += similarity * (neighbor_rating - user_avgs[neighbor_id])\n",
        "        else:\n",
        "            # In this case we do not have the average\n",
        "            sum_scores += similarity * neighbor_rating\n",
        "        sum_weights += abs(similarity)\n",
        "        \n",
        "    neighborhood_weighted_avg = sum_scores/sum_weights\n",
        "    \n",
        "    if with_deviations:\n",
        "        prediction = user_avgs[u_id] + neighborhood_weighted_avg\n",
        "        print(\"prediction\", prediction, \"(user_avg\", user_avgs[u_id], \", offset\", neighborhood_weighted_avg, \")\")\n",
        "    else:\n",
        "        prediction = neighborhood_weighted_avg\n",
        "        print(\"prediction\", prediction, \"(user_avg\", user_avgs[u_id], \")\")\n",
        "        \n",
        "    return prediction"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MMa5Di5ImMaA"
      },
      "source": [
        "**DEBUG:** For the truncated dataset, the following should output sth like:\n",
        "```\n",
        "Movie name: Sudden Death (1995)\n",
        "user 6 has not rated item 8\n",
        "k: 50 with_deviations: True with_abs_sim: True\n",
        "prediction 2.7554307504988773 (user_avg 3.2830188679245285 offset -0.527588117425651 )\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k4s_6CFfmMaA"
      },
      "source": [
        "if DEBUG:\n",
        "    k = 50\n",
        "    with_deviations = True\n",
        "    with_abs_sim = True\n",
        "    predict_rating(6, 8, k=k, with_deviations=with_deviations, with_abs_sim=with_abs_sim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KlBw9NYKmMaA"
      },
      "source": [
        "# Compute prediction of user 2 for Jurassic Park\n",
        "if DEBUG:\n",
        "    k = 50\n",
        "    with_deviations = True\n",
        "    with_abs_sim = True\n",
        "    predict_rating(2, movieId_to_movieIDX[480], k=k, with_deviations=with_deviations, with_abs_sim=with_abs_sim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rVp7u2nVzfZX"
      },
      "source": [
        "ratings[ratings.user == 2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KtAlGo3KzqmG"
      },
      "source": [
        "# Compute prediction of user 2 for movie with idx 157 - true value = 3\n",
        "if DEBUG:\n",
        "    k = 50\n",
        "    with_deviations = True\n",
        "    with_abs_sim = True\n",
        "    predict_rating(2, 157, k=k, with_deviations=with_deviations, with_abs_sim=with_abs_sim)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eLWjvjQc1MjX"
      },
      "source": [
        "### 1.12 Recommendation Exercise\n",
        "\n",
        "To do in groups (15 minutes): follow the steps and send your answers and code @Luc Kunz on Slack (direct message). One answer per person. Deadline: tomorrow (Dec 8, 2020) at 6pm. This is a good way to improve your participation grade.\n",
        "\n",
        "The idea is to recommend 5 movies to a user. Of course, the user must not have already watched the movies.\n",
        "\n",
        "Complete the code below when you see `# [YOUR CODE HERE]`.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DsJBSwE2-XGD"
      },
      "source": [
        "# recall - ratings\n",
        "ratings"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Puvko-HOw7yT"
      },
      "source": [
        "# 1. Choose a user (IDX) - between 0 and 9923.\n",
        "user_IDX = # [YOUR CODE HERE]\n",
        "\n",
        "# 2. Print user ID (hint: use userIDX_to_userId)\n",
        "# [YOUR CODE HERE]\n",
        "\n",
        "# 3. Display movies that user has already watched (hint: the function to use is defined above)\n",
        "# [YOUR CODE HERE]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eHUdN39-2NMU"
      },
      "source": [
        "# 4. Show genre histogram of the user (hint: the function to use is defined above)\n",
        "\n",
        "# 5. Among the 50 first items (IDX), which 5 would you recommend? Of course, the user should not have already watched the movies...\n",
        "# Parameters to use: k=50, with_deviation=True, with_abs_sim=False\n",
        "# We first create a dictionary (i_id, rating) with the predicted ratings for the movies\n",
        "ratings_exercise = {}\n",
        "for i_id in range(50):\n",
        "  if # [YOUR CODE HERE]: # user should not have already watched the movie (hint: use the R_dok matrix)\n",
        "    ratings_exercise[i_id] = # [YOUR CODE HERE] # predict rating of user for movie i_id (hint: use one of the above-defined functions)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "beXOIKyx-zPF"
      },
      "source": [
        "# Print dictionary\n",
        "ratings_exercise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ge_nCTGFAN00"
      },
      "source": [
        "# Sort dictionary and print top 5 movies\n",
        "top_ratings = sorted(ratings_exercise.values(), reverse=True)[:5] # first 5 movies\n",
        "for idx, rating in ratings_exercise.items():  \n",
        "  if rating in top_ratings:\n",
        "    print(get_name_for_movie_id(movieIDX_to_movieId[idx]))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UFH1wqizOom"
      },
      "source": [
        "# 6. Show covers of the movies. This question is difficult and therefore optional. You will also get the point without it.\n",
        "# [YOUR CODE HERE]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ynERFI7LQDlO"
      },
      "source": [
        "## 2. Using the Surprise library\n",
        "\n",
        "[Surprise](http://surpriselib.com/) is a Python scikit building and analyzing recommender systems that deal with explicit rating data. Its name stands for Simple Python RecommendatIon System Engine."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Amt3qWkcQwT-"
      },
      "source": [
        "# Install package\n",
        "!pip install surprise"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JS93fQN_QI9E"
      },
      "source": [
        "# Import packages\n",
        "from surprise import KNNBasic, KNNWithMeans\n",
        "from surprise import Dataset\n",
        "from collections import defaultdict\n",
        "from surprise import get_dataset_dir\n",
        "from surprise.model_selection import train_test_split\n",
        "import io"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wuQL8v0fQ6AD"
      },
      "source": [
        "Below we will create a KNN model using the 100k dataset from MovieLens. Information about the dataset is available [here](https://www.kaggle.com/prajitdatta/movielens-100k-dataset/). Documentation on the algorithm is available [here](https://surprise.readthedocs.io/en/stable/knn_inspired.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ti2xLXcu1guI"
      },
      "source": [
        "# Load data\n",
        "data = Dataset.load_builtin('ml-100k') # there are a couple of famous Rec System datasets available in this library\n",
        "trainset = data.build_full_trainset()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EQBN7Uv61sUK"
      },
      "source": [
        "# Number of movies\n",
        "trainset.all_items() "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3dnWyzA1x1E"
      },
      "source": [
        "# Number of users\n",
        "trainset.all_users()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3aKb5IkH1bIQ"
      },
      "source": [
        "# Define options and create instance of class\n",
        "sim_options = {\n",
        "    'name': 'pearson', # let's use pearson similarity which can be seen as mean-centered cosine similarity\n",
        "    'user_based': True # we will do user-based CF\n",
        "}\n",
        "knn_means = KNNWithMeans(k=40, min_k=1, sim_options=sim_options, verbose=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v83f8VHP1bGj"
      },
      "source": [
        "# Fit model\n",
        "knn_means.fit(trainset)\n",
        "\n",
        "# Predict ratings for all pairs (u, i) that are NOT in the training set.\n",
        "testset = trainset.build_anti_testset() \n",
        "predictions = knn_means.test(testset)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4XEJj5f82-6g"
      },
      "source": [
        "# Examples of predictions\n",
        "predictions[600:610]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O_rhhcdM33s1"
      },
      "source": [
        "So in less than 10 lines, we managed to do the same as in the first part... I love packages :)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jj6JlpMNRUG7"
      },
      "source": [
        "### 2.1 Some helper functions\n",
        "\n",
        "We have built the predictions. Now we can visualize them. We first write these helpers functions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oFVFOHjnRMBo"
      },
      "source": [
        "def read_item_names():\n",
        "    '''Read the u.item file from MovieLens 100-k dataset and return two\n",
        "    mappings to convert raw ids into movie names and movie names into raw ids.\n",
        "    '''\n",
        "\n",
        "    file_name = get_dataset_dir() + '/ml-100k/ml-100k/u.item'\n",
        "    rid_to_name = {}\n",
        "    name_to_rid = {}\n",
        "    with io.open(file_name, 'r', encoding='ISO-8859-1') as f:\n",
        "        for line in f:\n",
        "            line = line.split('|')\n",
        "            rid_to_name[line[0]] = line[1]\n",
        "            name_to_rid[line[1]] = line[0]\n",
        "\n",
        "    return rid_to_name, name_to_rid\n",
        "\n",
        "\n",
        "def get_top_n(predictions, n=10):\n",
        "    '''Return the top-N recommendation for each user from a set of predictions.\n",
        "\n",
        "    Args:\n",
        "        predictions(list of Prediction objects): The list of predictions, as\n",
        "            returned by the test method of an algorithm.\n",
        "        n(int): The number of recommendation to output for each user. Default\n",
        "            is 10.\n",
        "\n",
        "    Returns:\n",
        "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
        "        [(raw item id, rating estimation), ...] of size n.\n",
        "    '''\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    top_n = defaultdict(list) # This is used to group a sequence of key-value pairs into a dictionary of lists\n",
        "    for uid, iid, true_r, est, _ in predictions:\n",
        "        top_n[uid].append((iid, est))\n",
        "\n",
        "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
        "    for uid, user_ratings in top_n.items():\n",
        "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
        "        top_n[uid] = user_ratings[:n]\n",
        "\n",
        "    return top_n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yGCx7IflRsgR"
      },
      "source": [
        "### 2.2 Visualizing the recommendations\n",
        "\n",
        "We can see for each user what are the top recommended movies."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fs5qEf-xRqu5"
      },
      "source": [
        "# Get top 10 movies for all users\n",
        "top_n = get_top_n(predictions)\n",
        "\n",
        "# Top 10 movies for user 196\n",
        "top_n['196']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtlypKiCRkyx"
      },
      "source": [
        "# Show covers\n",
        "import re\n",
        "from IPython.display import Image\n",
        "from IPython.display import display, HTML\n",
        "\n",
        "top_n = get_top_n(predictions, n=10)\n",
        "\n",
        "# Read the mappings raw id <-> movie name\n",
        "rid_to_name, name_to_rid = read_item_names()\n",
        "\n",
        "# Print the recommended items for user id 1\n",
        "uid = '196'\n",
        "user_ratings = top_n[uid]\n",
        "recommended_items = [iid for (iid, _) in user_ratings]\n",
        "\n",
        "print('User id and list of item id:')\n",
        "print(uid, recommended_items)\n",
        "\n",
        "# Convert ids into names\n",
        "item_names = [rid_to_name[rid]\n",
        "              for rid in recommended_items]\n",
        "\n",
        "print('\\nUser id and list of item names:')\n",
        "print(uid, item_names)\n",
        "\n",
        "print('\\nMovies list:')\n",
        "# Show name, url and covers\n",
        "for name in item_names:\n",
        "    print('\\nName: ', name)\n",
        "    clean_name = re.sub(r'\\([^)]*\\)', '', name) # this remove the year of the movie which is in between paranthesis\n",
        "    try:\n",
        "      url = week11.TMDB().get_poster_path_by_name(clean_name)\n",
        "    except:\n",
        "      url = None\n",
        "    print('url: ', url)\n",
        "    if url:\n",
        "        display(Image(url=url))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwLE-UGxR9T2"
      },
      "source": [
        "### 2.3 Cross-validation using surprise\n",
        "This package also provides for you built-in cross-validation to split the data to multiple folds."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qj7695zqR03K"
      },
      "source": [
        "from surprise.model_selection import cross_validate\n",
        "from surprise import SVD\n",
        "\n",
        "algo = SVD()\n",
        "\n",
        "# Run 5-fold cross-validation and print results.\n",
        "cross_validate(algo, data, measures=['RMSE'], cv=5, verbose=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Oe0jsWpdSDcW"
      },
      "source": [
        "#### User-based collaborative filtering with surprise!"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M33op77uSAhX"
      },
      "source": [
        "from surprise import KNNWithMeans\n",
        "\n",
        "sim_options = {\n",
        "    'name': 'pearson', #let's use pearson similarity which can be seen as mean-centered cosine similarity\n",
        "    'user_based': True #we will do user-based CF\n",
        "}\n",
        "knn_means = KNNWithMeans(k=40, min_k=1, sim_options=sim_options, verbose=False)\n",
        "\n",
        "# Run 5-fold cross-validation and print results.\n",
        "cross_validate(knn_means, data, measures=['RMSE'], cv=5, verbose=True);"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPus0WuNQTu-"
      },
      "source": [
        "### 2.4 Precision - Recall @k"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YygiDmbSGeC"
      },
      "source": [
        "def precision_recall_at_k(predictions, k=10, threshold=3.5):\n",
        "    '''Return precision and recall at k metrics for each user.'''\n",
        "\n",
        "    # First map the predictions to each user.\n",
        "    user_est_true = defaultdict(list)\n",
        "    for uid, _, true_r, est, _ in predictions:\n",
        "        user_est_true[uid].append((est, true_r))\n",
        "\n",
        "    precisions = dict()\n",
        "    recalls = dict()\n",
        "    for uid, user_ratings in user_est_true.items():\n",
        "\n",
        "        # Sort user ratings by estimated value\n",
        "        user_ratings.sort(key=lambda x: x[0], reverse=True)\n",
        "\n",
        "        # Number of relevant items\n",
        "        n_rel = sum((true_r >= threshold) for (_, true_r) in user_ratings)\n",
        "\n",
        "        # Number of recommended items in top k\n",
        "        n_rec_k = sum((est >= threshold) for (est, _) in user_ratings[:k])\n",
        "\n",
        "        # Number of relevant and recommended items in top k\n",
        "        n_rel_and_rec_k = sum(((true_r >= threshold) and (est >= threshold))\n",
        "                              for (est, true_r) in user_ratings[:k])\n",
        "\n",
        "        # Precision@K: Proportion of recommended items that are relevant\n",
        "        precisions[uid] = n_rel_and_rec_k / n_rec_k if n_rec_k != 0 else 1\n",
        "\n",
        "        # Recall@K: Proportion of relevant items that are recommended\n",
        "        recalls[uid] = n_rel_and_rec_k / n_rel if n_rel != 0 else 1\n",
        "\n",
        "    return precisions, recalls"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHK1LMXYSPis"
      },
      "source": [
        "trainset, testset = train_test_split(data, test_size=0.2)\n",
        "algo = SVD()\n",
        "\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "precision = []\n",
        "recall = []\n",
        "for k in range(10):\n",
        "    precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=3.5) # rating > 3 = relevant, rating < 3 = irrelevant\n",
        "\n",
        "    # Precision and recall can then be averaged over all users\n",
        "    precision.append( sum(prec for prec in precisions.values()) / len(precisions) )\n",
        "    recall.append( sum(rec for rec in recalls.values()) / len(recalls) )\n",
        "\n",
        "plt.plot(range(10), recall, 'ro-', label=\"recall\")\n",
        "plt.plot(range(10), precision, 'go-', label=\"precision\")\n",
        "plt.title(\"precision and recall for SVD\")\n",
        "plt.legend()\n",
        "plt.show();\n",
        "\n",
        "precisions, recalls = precision_recall_at_k(predictions, k=20, threshold=3.5)\n",
        "\n",
        "print(\"precision @ 20 for SVD\", sum(prec for prec in precisions.values()) / len(precisions))\n",
        "print(\"recall @ 20 for SVD\", sum(rec for rec in recalls.values()) / len(recalls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t34ttk2LSRiP"
      },
      "source": [
        "algo = KNNWithMeans(k=40, min_k=1, sim_options=sim_options, verbose=False)\n",
        "\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "precision = []\n",
        "recall = []\n",
        "for k in range(10):\n",
        "    precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=3.5) # rating > 3 = relevant, rating < 3 = irrelevant\n",
        "\n",
        "    # Precision and recall can then be averaged over all users\n",
        "    precision.append( sum(prec for prec in precisions.values()) / len(precisions) )\n",
        "    recall.append( sum(rec for rec in recalls.values()) / len(recalls) )\n",
        "\n",
        "\n",
        "plt.plot(range(10), recall, 'ro-', label=\"recall\")\n",
        "plt.plot(range(10), precision, 'go-', label=\"precision\")\n",
        "plt.legend()\n",
        "plt.title(\"precision and recall for user-based knn\")\n",
        "plt.show();\n",
        "\n",
        "precisions, recalls = precision_recall_at_k(predictions, k=20, threshold=3.5)\n",
        "\n",
        "print(\"precision @ 20 for user-based knn\", sum(prec for prec in precisions.values()) / len(precisions))\n",
        "print(\"recall @ 20 for user-based knn\", sum(rec for rec in recalls.values()) / len(recalls))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Dl6U1X9tSWS2"
      },
      "source": [
        "#### Precision-recall curve\n",
        "We will now observe the area under precision recall curve for tow methods: SVD and KNN."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pLLQAQ3JST84"
      },
      "source": [
        "from inspect import signature\n",
        "\n",
        "algo = SVD()\n",
        "\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "precision = []\n",
        "recall = []\n",
        "for k in range(20):\n",
        "    precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=3.5) \n",
        "\n",
        "    # Precision and recall can then be averaged over all users\n",
        "    precision.append( sum(prec for prec in precisions.values()) / len(precisions) )\n",
        "    recall.append( sum(rec for rec in recalls.values()) / len(recalls) )\n",
        "\n",
        "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
        "step_kwargs = ({'step': 'post'}\n",
        "               if 'step' in signature(plt.fill_between).parameters\n",
        "               else {})\n",
        "plt.step(recall, precision, color='b', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall curve for SVD');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "arIFHLFMSZa7"
      },
      "source": [
        "algo =  KNNWithMeans(k=40, min_k=1, sim_options=sim_options, verbose=False)\n",
        "\n",
        "algo.fit(trainset)\n",
        "predictions = algo.test(testset)\n",
        "\n",
        "precision = []\n",
        "recall = []\n",
        "for k in range(20):\n",
        "    precisions, recalls = precision_recall_at_k(predictions, k=k, threshold=3.5) \n",
        "\n",
        "    # Precision and recall can then be averaged over all users\n",
        "    precision.append( sum(prec for prec in precisions.values()) / len(precisions) )\n",
        "    recall.append( sum(rec for rec in recalls.values()) / len(recalls) )\n",
        "\n",
        "# In matplotlib < 1.5, plt.fill_between does not have a 'step' argument\n",
        "step_kwargs = ({'step': 'post'}\n",
        "               if 'step' in signature(plt.fill_between).parameters\n",
        "               else {})\n",
        "plt.step(recall, precision, color='b', alpha=0.2,\n",
        "         where='post')\n",
        "plt.fill_between(recall, precision, alpha=0.2, color='b', **step_kwargs)\n",
        "\n",
        "plt.xlabel('Recall')\n",
        "plt.ylabel('Precision')\n",
        "plt.title('Precision-Recall curve for user-based KNN');"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B7MDnHBBSdhi"
      },
      "source": [
        "### 2.5 Tuning hyper-parameters with surprise\n",
        "As we saw we have built-in cross-validation in this package. We can use this to tune the hyper-parameters of our recommender system, eg tuning the number of neighbours in KNN or the number of factors (or the reduced dimensionality) in SVD."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QAbGgPd-SbKh"
      },
      "source": [
        "from surprise.model_selection import GridSearchCV\n",
        "SVD_grid_search = GridSearchCV(SVD, param_grid={'n_factors': [50, 100, 200, 300]}, measures=['RMSE'], cv=5,\n",
        "                               refit=True, joblib_verbose=2, n_jobs=-1)\n",
        "SVD_grid_search.fit(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kphNP84fSiL2"
      },
      "source": [
        "print(\"best parameter:\", SVD_grid_search.best_params)\n",
        "print(\"best rmse: \", SVD_grid_search.best_score)\n",
        "# you can even see the whole cv results\n",
        "print(\"\\n\")\n",
        "SVD_grid_search.cv_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qijBtDFGSmPa"
      },
      "source": [
        "Now for KNN"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7XJ2HIxaSjzt"
      },
      "source": [
        "print(sim_options)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hst8xKOvSokf"
      },
      "source": [
        "KNN_grid_search = GridSearchCV(KNNWithMeans, param_grid={'k': [20, 30, 40, 50], \n",
        "                                                         'sim_options': {'name': ['pearson'], 'user_based': [True]}}, \n",
        "                               measures=['RMSE'], cv=5,\n",
        "                               refit=True, joblib_verbose=2, n_jobs=-1)\n",
        "KNN_grid_search.fit(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nbzLXSHxSp_X"
      },
      "source": [
        "print(\"best parameter:\", KNN_grid_search.best_params)\n",
        "print(\"best rmse: \", KNN_grid_search.best_score)\n",
        "# you can even see the whole cv results\n",
        "print(\"\\n\")\n",
        "KNN_grid_search.cv_results"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N7WFcS8eStYP"
      },
      "source": [
        "Let's save the best SVD model. We will use it later."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPD_sr1mSrW2"
      },
      "source": [
        "best_model_svd = SVD_grid_search.best_estimator['rmse']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Xj9Uiq7Svxm"
      },
      "source": [
        "import pickle\n",
        "file_name = \"best_model_svd\"\n",
        "pickle.dump(best_model_svd, open(file_name, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fzauiinvSxTn"
      },
      "source": [
        "m = pickle.load(open(\"best_model_svd\", 'rb'))\n",
        "m.predict('6', '908')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sz6kxaOT70Al"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}